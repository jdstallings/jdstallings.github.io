<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="google-site-verification" content="jJxC4Er7z420wuGCy9NA_ulVs0MEfqQ9HdgCu9Zfado" />
  <meta charset="utf-8">
	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author"      content="Sergey Pozhilov (GetTemplate.com)">

	<title>Linear Regression</title>

	<link rel="shortcut icon" href="/images/gt_favicon.png">

	
	<link href="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css" rel="stylesheet">
	
	<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
	
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
	
	<link rel="stylesheet" href="/css/styles.css">

	

    
        <script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5a67c0296f97ab00136db490&product=inline-share-buttons"></script>
    

</head>
<body class="home">

<header id="header">
	<div id="head" class="parallax" parallax-speed="2">
		<h1 id="logo" class="text-center">
			<img class="img-circle" src="/images/guy.jpg" alt="">
			<span class="title">Jonathan D. Stallings</span>
			<span class="tagline">An Aspiring Data Scientist<br>
				<a href="">jdstallings@DataInDeed.io</a>
            </span>
		</h1>
	</div>

    <nav class="navbar navbar-default navbar-sticky">
    <div class="container-fluid">

        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="true">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
        </div>

        <div class="navbar-collapse collapse" id="bs-example-navbar-collapse-1">

            <ul class="nav navbar-nav">
                
                <li>
                    <a href="/">home</a>
                </li>
                
                <li>
                    <a href="/about/">about</a>
                </li>
                
                <li>
                    <a href="/post/">blog</a>
                </li>
                
                
            </ul>

        </div>
    </div>
</nav>


</header>


<main id="main">

	<div class="container">

		<div class="row topspace">
			<div class="col-sm-8 col-sm-offset-2">

 				<article class="post">
					<header class="entry-header">
 						<div class="entry-meta">
 							<span class="posted-on"><time class="entry-date published" date="2017-01-14 00:00:00 &#43;0000 UTC">14 January 2017</time></span>
 						</div>
 						<h1 class="entry-title"><a href="/2017/01/14/linear-regression/" rel="bookmark">Linear Regression</a></h1>
					</header>
					<div class="entry-content">
						<p>Linear regression is a very simple approach for supervised learning in order to find relationship between a single, continuous variable called dependent (or target) variable (i.e. numeric values, not categorical or groups) and one or more other variables (continuous or factor/groups) called independent variables. Thus, it can be used to predict a quantitative response. At least 5 cases per independent variable in the analysis is required. Many of the more sophisticated statistical learning approaches are in fact generalizations or extensions of linear regression.</p>
<hr />
<p>The major resources used for this post include: + <a href="https://www.amazon.com/gp/product/B000TQ6FJ0/ref=kinw_myk_ro_title">‘Introductory Statistics with R’ by Peter Dalgaard</a> + <a href="https://www.amazon.com/gp/product/B00HPZ4VVM/ref=kinw_myk_ro_title">‘Discovering Statistics Using R’ by</a> + <a href="https://www.amazon.com/gp/product/B00DM0VX60/ref=kinw_myk_ro_title">‘An Introduction to Statistical Learning’ by Gereth James, et al.</a> + <a href="https://www.amazon.com/gp/product/B00E3UR6IM/ref=kinw_myk_ro_title">‘Statistical Models: Theory and Practice’ by David Freedman</a> + <a href="https://www.amazon.com/gp/product/B00INYG5I6/ref=kinw_myk_ro_title">‘Statistical Models’ by A.C. Davison</a> + And several blogs that address this topic, see <a href="http://www.datasciencecentral.com/profiles/blogs/a-complete-tutorial-on-linear-regression-with-r">here</a>. - Linear regression graphs a line over a set of data points that most closely fits the overall shape of the data in order to show the extent to which changes in a dependent variable (y axis) are attributed to changes in an independent variable (x axis). - Example equation: <span class="math inline">\(Y = b_0 + b_1X_1 + b_2X_2 + b_3X_3 + ...... + b_kX_k\)</span>, where <span class="math inline">\(b_0\)</span> is the intercept of the expected mean value of dependent variable (<span class="math inline">\(Y\)</span>) when all independent variables (<span class="math inline">\(X\)</span>) are equal to 0. <span class="math inline">\(b_1\)</span> is the slope, which represents the amount by which the dependent variable (<span class="math inline">\(Y\)</span>) changes if <span class="math inline">\(X_1\)</span> was changed by one unit, while keeping other variables constant. - Important term: <em>Residual.</em> The difference between observed values of the dependent variable and the value of the dependent variable predicted are residuals that form the regression line. - Algorithm: Linear regression is based on least square estimation, which indicates regression coefficients (estimates) should be chosen in a manner to minimize the sum of the squared distances of each observed response to its fitted value (i.e. the least amount of information loss, see below.)</p>
<div id="example-use-in-business" class="section level2">
<h2>Example Use in Business:</h2>
<div id="evaluating-trends-and-sales-estimates" class="section level3">
<h3>Evaluating Trends and Sales Estimates</h3>
<ul>
<li>If a company’s sales have increased steadily every month for the past few years, conducting a linear analysis on the sales data with monthly sales on the y-axis and time on the x-axis would produce a line that that depicts the upward trend in sales. After creating the trend line, the company could use the slope of the line to forecast sales in future months.</li>
</ul>
</div>
<div id="analyzing-the-impact-of-pricing-changes" class="section level3">
<h3>Analyzing the Impact of Pricing Changes</h3>
<ul>
<li>If a company changes the price on a certain product several times, it can record the quantity it sells for each price level and then perform a linear regression with quantity sold as the dependent variable and price as the explanatory variable. The result would be a line that depicts the extent to which consumers reduce their consumption of the product as prices increase, which could help guide future pricing decisions.</li>
</ul>
</div>
<div id="assessing-risk" class="section level3">
<h3>Assessing Risk</h3>
<ul>
<li>A health insurance company might conduct a linear regression plotting number of claims per customer against age and discover that older customers tend to make more health insurance claims. The results of such an analysis might guide important business decisions made to account for risk.</li>
</ul>
</div>
</div>
<div id="assumptions" class="section level2">
<h2>Assumptions</h2>
<ul>
<li><em>Linear Relationship:</em> A linear relationship between the dependent and independent variable. Prior to conducting linear regression, you should explore the data by plotting the independent variable vs. dependent variables and run correlation between dependent variable and independent variables. There should be moderate and significant correlation score between them.
<ul>
<li>When the error variance appears to be constant (homoscedasticity), only <span class="math inline">\(X\)</span> needs be transformed to linearize the relationship.</li>
<li>Transform independent variable to <span class="math inline">\(\log_{10} X\)</span>, <span class="math inline">\(f^{-1} X\)</span>, <span class="math inline">\(\sqrt X\)</span>, <span class="math inline">\(X^2\)</span>, <span class="math inline">\(\exp X\)</span>, <span class="math inline">\(1/X\)</span>, <span class="math inline">\(\exp (-X)\)</span>. When the error variance does not appear constant it may be necessary to transform <span class="math inline">\(Y\)</span> or both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> (see below).</li>
</ul></li>
<li><em>Normaility of Residual:</em> Linear regression requires residuals (the difference between an observed value of the dependent variable and the value of the dependent variable predicted from the regression line) should be normality distributed.
<ul>
<li>Standardized Residuals is the residuals divided by the standard error of estimate (<span class="math inline">\(\sigma_{est} = \sqrt{\frac{\sum(Y-Y&#39;)^2}{N}}\)</span>). It is a measure of accuracy of predictors.</li>
<li>Studentized Residuals is the residuals divided by the residual standard error (<span class="math inline">\(RSE = \sqrt{\frac{\sum\hat e_i^2}{n-2}}\)</span>) with that case deleted, where <span class="math inline">\(e\)</span> is the residual error. If the absolute value of studentized residual is $ &gt; 3$, the observation is considered as an outlier.</li>
<li>Models with and without outliers have to be compared, and the smaller the the square root of the variance of the residuals, <span class="math inline">\(RMSE = \sqrt{\frac{\sum_{i-1}^n(X_{obs,i}-X_{model,i})^2}{n}}\)</span>, the better the model. It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values. Whereas <span class="math inline">\(R^2 = (\frac{\sum_{i=1}^n(x_i - \overline x) \cdot (y_i - \overline y)}{\sqrt{\sum_{i=1}^n(x_i - \overline x)^2 \cdot \sum_{i=1}^n(y_i - \overline y)^2}})^2\)</span> is a relative measure of fit, <span class="math inline">\(RMSE\)</span> is an absolute measure of fit. Lower values of <span class="math inline">\(RMSE\)</span> indicate better fit.</li>
</ul></li>
<li><p><em>Homoscedasticity:</em> Linear regression assumes that residuals are approximately equal for all predicted dependent variable values.</p></li>
<li><em>No Outlier Problem:</em> By using the box plot method to identify any values that are <span class="math inline">\(1.5 \times IQR\)</span> above the upper quartile (<span class="math inline">\(Q3\)</span>) or below the lower quartile (<span class="math inline">\(Q1\)</span>).
<ul>
<li>Percentile capping based on distribution of a variable is used to deal with outliers by replacing extreme values with the largest/smallest non-extreme observation.</li>
</ul></li>
<li><p><em>Multicollinearity:</em> It means there is a high correlation between independent variables. The linear regression model MUST NOT be faced with problem of multicollinearity.</p></li>
<li><p><em>Independence of error terms - No Autocorrelation:</em> It states that the errors associated with one observation are not correlated with the errors of any other observation. It is a problem when you use time series data. Suppose you have collected data from labors in eight different districts. It is likely that the labors within each district will tend to be more like one another that labors from different districts, that is, their errors are not independent.</p></li>
</ul>
</div>
<div id="example-analysis" class="section level2">
<h2>Example Analysis</h2>
<div id="downloading-data-and-evaluating-linear-relationships" class="section level3">
<h3>Downloading Data and Evaluating Linear Relationships</h3>
<ul>
<li>An internal R data set called ‘mtcars’ was used to demonstrate the linear regression and resulting analysis.</li>
<li>To examine the linear relationships we consider the following:
<ul>
<li>Pearson correlation measures linear relationship of variables at interval scales, and is sensitive to outliers. A Pearson correlation of <em>1</em> indicates a perfect correlation, <em>0</em> indicates no linear relationship.</li>
<li>Spearman’s correlation measures monotonic relationship, used for ordinal variables, and is less sensitive to outliers. Scores close to <em>0</em> indicate no monotonic relationship between variables.</li>
<li>Hoeffding’s D correlation measures linear, monotonic and non-monotonic relationship. It has values between –0.5 to 1. The signs of Hoeffding coefficient has no interpretation.</li>
<li>If a variable has a very low rank for Spearman (coefficient - close to 0) and a very high rank for Hoeffding indicates a non-monotonic relationship.</li>
<li>If a variable has a very low rank for Pearson (coefficient - close to 0) and a very high rank for Hoeffding indicates a non-linear relationship.</li>
<li>If a variable has poor rank on both the Spearman and Hoeffding correlation metrics, it means the relationship between the variables is random.</li>
</ul></li>
</ul>
<pre class="r"><code>library(ggplot2)
library(car)
library(caret)
library(data.table)
library(pander)

# Loading data
data(mtcars)
# Place data in a data.table with rownames
model &lt;- data.table(model = rownames(mtcars))
model &lt;- cbind(model, mtcars)

# Convert categorical variables to factor
model &lt;- model[,&quot;am&quot; := as.factor(am)]
model &lt;- model[,&quot;cyl&quot; := as.factor(cyl)]
model &lt;- model[,&quot;vs&quot; := as.factor(vs)]
model &lt;- model[,&quot;gear&quot; := as.factor(gear)]
model &lt;- model[,&quot;model&quot; := as.factor(model)]

# Dropping dependent variable
model_dep &lt;- subset(model, select = -c(mpg))

# Identifying numeric variables
numData &lt;- model_dep[,sapply(model_dep, is.numeric), with = FALSE]
panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(head(model))</code></pre>
<table>
<caption>Table continues below</caption>
<thead>
<tr class="header">
<th align="center">model</th>
<th align="center">mpg</th>
<th align="center">cyl</th>
<th align="center">disp</th>
<th align="center">hp</th>
<th align="center">drat</th>
<th align="center">wt</th>
<th align="center">qsec</th>
<th align="center">vs</th>
<th align="center">am</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Mazda RX4</td>
<td align="center">21</td>
<td align="center">6</td>
<td align="center">160</td>
<td align="center">110</td>
<td align="center">3.9</td>
<td align="center">2.62</td>
<td align="center">16.46</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">Mazda RX4 Wag</td>
<td align="center">21</td>
<td align="center">6</td>
<td align="center">160</td>
<td align="center">110</td>
<td align="center">3.9</td>
<td align="center">2.875</td>
<td align="center">17.02</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">Datsun 710</td>
<td align="center">22.8</td>
<td align="center">4</td>
<td align="center">108</td>
<td align="center">93</td>
<td align="center">3.85</td>
<td align="center">2.32</td>
<td align="center">18.61</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">Hornet 4 Drive</td>
<td align="center">21.4</td>
<td align="center">6</td>
<td align="center">258</td>
<td align="center">110</td>
<td align="center">3.08</td>
<td align="center">3.215</td>
<td align="center">19.44</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
<tr class="odd">
<td align="center">Hornet Sportabout</td>
<td align="center">18.7</td>
<td align="center">8</td>
<td align="center">360</td>
<td align="center">175</td>
<td align="center">3.15</td>
<td align="center">3.44</td>
<td align="center">17.02</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr class="even">
<td align="center">Valiant</td>
<td align="center">18.1</td>
<td align="center">6</td>
<td align="center">225</td>
<td align="center">105</td>
<td align="center">2.76</td>
<td align="center">3.46</td>
<td align="center">20.22</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th align="center">gear</th>
<th align="center">carb</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">4</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="center">4</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<ul>
<li>First, lets visualize the relationships with ggplot2:</li>
</ul>
<pre class="r"><code>fit &lt;- lm(mpg ~ disp, data = model) # test linear relationship of mpg and disp
r2 &lt;- paste(&quot;R^2:&quot;, format(summary(fit)$adj.r.squared, digits = 4))

g &lt;- ggplot(model, aes(x = disp, y = mpg)) + geom_point()
g &lt;- g + geom_smooth(method = &quot;lm&quot;, se = T)
g &lt;- g + annotate(&quot;text&quot;, x = 400, y = 30, label = r2)
g </code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-2-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>p.cor &lt;- cor(model$disp, model$mpg, method = c(&quot;pearson&quot;))
s.cor &lt;- cor(model$disp, model$mpg, method = c(&quot;spearman&quot;))
library(Hmisc)
h.cor &lt;- hoeffd(model$disp, model$mpg)
cor &lt;- data.frame(&quot;correlation scores&quot; = c(p.cor, s.cor, h.cor$D[1,2]))
rownames(cor) &lt;- c(&quot;pearson&quot;, &quot;spearman&quot;, &quot;hoeffd&quot;)
panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(cor)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">correlation.scores</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>pearson</strong></td>
<td align="center">-0.8476</td>
</tr>
<tr class="even">
<td align="center"><strong>spearman</strong></td>
<td align="center">-0.9089</td>
</tr>
<tr class="odd">
<td align="center"><strong>hoeffd</strong></td>
<td align="center">0.4733</td>
</tr>
</tbody>
</table>
<pre class="r"><code>fit &lt;- lm(mpg ~ hp, data = model) # test linear relationship of mpg and hp
r2 &lt;- paste(&quot;R^2:&quot;, format(summary(fit)$adj.r.squared, digits = 4))

g &lt;- ggplot(model, aes(x = hp, y = mpg)) + geom_point()
g &lt;- g + geom_smooth(method = &quot;lm&quot;, se = T)
g &lt;- g + annotate(&quot;text&quot;, x = 300, y = 30, label = r2)
g</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-3-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>p.cor &lt;- cor(model$hp, model$mpg, method = c(&quot;pearson&quot;))
s.cor &lt;- cor(model$hp, model$mpg, method = c(&quot;spearman&quot;))
library(Hmisc)
h.cor &lt;- hoeffd(model$hp, model$mpg)
cor &lt;- data.frame(&quot;correlation scores&quot; = c(p.cor, s.cor, h.cor$D[1,2]))
rownames(cor) &lt;- c(&quot;pearson&quot;, &quot;spearman&quot;, &quot;hoeffd&quot;)
panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(cor)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">correlation.scores</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>pearson</strong></td>
<td align="center">-0.7762</td>
</tr>
<tr class="even">
<td align="center"><strong>spearman</strong></td>
<td align="center">-0.8947</td>
</tr>
<tr class="odd">
<td align="center"><strong>hoeffd</strong></td>
<td align="center">0.4815</td>
</tr>
</tbody>
</table>
<pre class="r"><code>fit &lt;- lm(mpg ~ drat, data = model) # test linear relationship of mpg and drat
r2 &lt;- paste(&quot;R^2:&quot;, format(summary(fit)$adj.r.squared, digits = 4))

g &lt;- ggplot(model, aes(x = drat, y = mpg)) + geom_point()
g &lt;- g + geom_smooth(method = &quot;lm&quot;, se = T)
g &lt;- g + annotate(&quot;text&quot;, x = 4.5, y = 12.5, label = r2)
g</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-4-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>p.cor &lt;- cor(model$drat, model$mpg, method = c(&quot;pearson&quot;))
s.cor &lt;- cor(model$drat, model$mpg, method = c(&quot;spearman&quot;))
library(Hmisc)
h.cor &lt;- hoeffd(model$drat, model$mpg)
cor &lt;- data.frame(&quot;correlation scores&quot; = c(p.cor, s.cor, h.cor$D[1,2]))
rownames(cor) &lt;- c(&quot;pearson&quot;, &quot;spearman&quot;, &quot;hoeffd&quot;)
panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(cor)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">correlation.scores</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>pearson</strong></td>
<td align="center">0.6812</td>
</tr>
<tr class="even">
<td align="center"><strong>spearman</strong></td>
<td align="center">0.6515</td>
</tr>
<tr class="odd">
<td align="center"><strong>hoeffd</strong></td>
<td align="center">0.1251</td>
</tr>
</tbody>
</table>
<pre class="r"><code>fit &lt;- lm(mpg ~ wt, data = model) # test linear relationship of mpg and wt
r2 &lt;- paste(&quot;R^2:&quot;, format(summary(fit)$adj.r.squared, digits = 4))

g &lt;- ggplot(model, aes(x = wt, y = mpg)) + geom_point()
g &lt;- g + geom_smooth(method = &quot;lm&quot;, se = T)
g &lt;- g + annotate(&quot;text&quot;, x = 4.5, y = 30, label = r2)
g</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>p.cor &lt;- cor(model$wt, model$mpg, method = c(&quot;pearson&quot;))
s.cor &lt;- cor(model$wt, model$mpg, method = c(&quot;spearman&quot;))
library(Hmisc)
h.cor &lt;- hoeffd(model$wt, model$mpg)
cor &lt;- data.frame(&quot;correlation scores&quot; = c(p.cor, s.cor, h.cor$D[1,2]))
rownames(cor) &lt;- c(&quot;pearson&quot;, &quot;spearman&quot;, &quot;hoeffd&quot;)
panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(cor)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">correlation.scores</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>pearson</strong></td>
<td align="center">-0.8677</td>
</tr>
<tr class="even">
<td align="center"><strong>spearman</strong></td>
<td align="center">-0.8864</td>
</tr>
<tr class="odd">
<td align="center"><strong>hoeffd</strong></td>
<td align="center">0.4064</td>
</tr>
</tbody>
</table>
<pre class="r"><code>fit &lt;- lm(mpg ~ qsec, data = model) # test linear relationship of mpg and qsec
r2 &lt;- paste(&quot;R^2:&quot;, format(summary(fit)$adj.r.squared, digits = 4))

g &lt;- ggplot(model, aes(x = qsec, y = mpg)) + geom_point()
g &lt;- g + geom_smooth(method = &quot;lm&quot;, se = T)
g &lt;- g + annotate(&quot;text&quot;, x = 21.5, y = 12.5, label = r2)
g</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-6-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>p.cor &lt;- cor(model$qsec, model$mpg, method = c(&quot;pearson&quot;))
s.cor &lt;- cor(model$qsec, model$mpg, method = c(&quot;spearman&quot;))
library(Hmisc)
h.cor &lt;- hoeffd(model$qsec, model$mpg)
cor &lt;- data.frame(&quot;correlation scores&quot; = c(p.cor, s.cor, h.cor$D[1,2]))
rownames(cor) &lt;- c(&quot;pearson&quot;, &quot;spearman&quot;, &quot;hoeffd&quot;)
panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(cor)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">correlation.scores</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>pearson</strong></td>
<td align="center">0.4187</td>
</tr>
<tr class="even">
<td align="center"><strong>spearman</strong></td>
<td align="center">0.4669</td>
</tr>
<tr class="odd">
<td align="center"><strong>hoeffd</strong></td>
<td align="center">0.06585</td>
</tr>
</tbody>
</table>
<ul>
<li>If the assumption of linearity is violated, the linear regression model returns incorrect (i.e. biased) estimates, resulting in underestimated coefficients and <span class="math inline">\(R^2\)</span> values.</li>
</ul>
</div>
<div id="removing-highly-correlated-variables" class="section level3">
<h3>Removing Highly Correlated Variables</h3>
<ul>
<li>Removal of highly correlated variables reduces bias in the model.
<ul>
<li>Using the correlation function, and plot has a heat map, you can quickly see which variables are highly correlated (i.e. blue boxes).<br />
</li>
<li>Using the findCorrelation function, we can explicitly identify those variables that reach a certain threshold, such as a 0.6 cutoff.</li>
</ul></li>
</ul>
<pre class="r"><code>library(heatmap3)

# Calculating Correlation
descrCor &lt;- cor(numData)
highlyCorrelated &lt;- findCorrelation(descrCor, cutoff = 0.6)

# Identifying Variable Names of Highly Correlated Variables
highlyCorCol &lt;- colnames(numData)[highlyCorrelated]

# Print correlated attributes
heatmap3(descrCor)</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-7-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Remove highly correlated variables and create a new dataset
dat3 &lt;- model[, -which(colnames(model) %in% highlyCorCol), with = FALSE]</code></pre>
<ul>
<li>Here, I’ve specified the columns to use in the linear regression model. Alternatively, if you remove the ‘model’ columns earlier, you can just type mpg ~ . to conduct analysis on all the remaining columns in the data.table.</li>
</ul>
<pre class="r"><code># Build Linear Regression Model
fit = lm(mpg ~ cyl + drat + qsec + vs + am + gear, data = dat3)

# Check Model Performance
panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(summary(fit))</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">11.99</td>
<td align="center">21.35</td>
<td align="center">0.5618</td>
<td align="center">0.5797</td>
</tr>
<tr class="even">
<td align="center"><strong>cyl6</strong></td>
<td align="center">-4.379</td>
<td align="center">2.59</td>
<td align="center">-1.691</td>
<td align="center">0.1044</td>
</tr>
<tr class="odd">
<td align="center"><strong>cyl8</strong></td>
<td align="center">-7.179</td>
<td align="center">4.084</td>
<td align="center">-1.758</td>
<td align="center">0.09206</td>
</tr>
<tr class="even">
<td align="center"><strong>drat</strong></td>
<td align="center">1.42</td>
<td align="center">2.407</td>
<td align="center">0.59</td>
<td align="center">0.5609</td>
</tr>
<tr class="odd">
<td align="center"><strong>qsec</strong></td>
<td align="center">0.3208</td>
<td align="center">0.8395</td>
<td align="center">0.3821</td>
<td align="center">0.7059</td>
</tr>
<tr class="even">
<td align="center"><strong>vs1</strong></td>
<td align="center">1.584</td>
<td align="center">2.516</td>
<td align="center">0.6297</td>
<td align="center">0.5351</td>
</tr>
<tr class="odd">
<td align="center"><strong>am1</strong></td>
<td align="center">4.651</td>
<td align="center">2.622</td>
<td align="center">1.774</td>
<td align="center">0.08932</td>
</tr>
<tr class="even">
<td align="center"><strong>gear4</strong></td>
<td align="center">-2.246</td>
<td align="center">2.874</td>
<td align="center">-0.7817</td>
<td align="center">0.4424</td>
</tr>
<tr class="odd">
<td align="center"><strong>gear5</strong></td>
<td align="center">-2.412</td>
<td align="center">3.025</td>
<td align="center">-0.7971</td>
<td align="center">0.4336</td>
</tr>
</tbody>
</table>
<table>
<caption>Fitting linear model: mpg ~ cyl + drat + qsec + vs + am + gear</caption>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">32</td>
<td align="center">3.287</td>
<td align="center">0.7793</td>
<td align="center">0.7026</td>
</tr>
</tbody>
</table>
<ul>
<li>Additional information is extracted from the summary(fit) object with the $ operator, such as Coefficients (coeff), Rsquared values (r.squared), and adjusted Rsquared value (adj.r.square).</li>
</ul>
</div>
<div id="measuring-the-quality-of-the-statistical-models" class="section level3">
<h3>Measuring the Quality of the Statistical Models</h3>
<ul>
<li>The Akaike information criteria (AIC) measures relative quality of statistical models given a set of data, which provides for model selection. AIC deals with trade-offs between goodness of fit of the model and model complexity.<br />
</li>
<li>AIC calculates the <a href="http://projecteuclid.org/download/pdf_1/euclid.aoms/1177729694">Kullback-Leibler divergence</a>, <span class="math inline">\(D_{KL}(f||g_1)\)</span>, to assess the loss of information from model to model. The model that minimizes information loss is selected.
<ul>
<li>For more information see <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;ved=0ahUKEwjto7asoMDRAhWGWSYKHWZfDP4QFggiMAE&amp;url=http%3A%2F%2Fwww.sortie-nd.org%2Flme%2FStatistical%2520Papers%2FAkaike_1973%2520with%2520commentary.pdf&amp;usg=AFQjCNHQK_Ng1PBgZgv2TnOkTmX4C5DrFg&amp;bvm=bv.144224172,d.eWE">Akaike’s original 1973 manuscript</a>.</li>
<li>One limitation is small sample size, therefore, the estimate is only valid asymptotically. For smaller sample sizes, a correction (AICc) may be appropriate, see <a href="https://academic.oup.com/biomet/article-abstract/76/2/297/265326/Regression-and-time-series-model-selection-in?redirectedFrom=fulltext">here</a>.<br />
</li>
</ul></li>
<li>The Bayesian information criterion (BIC) or Schwarz criterion (SBC) is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. For more information see <a href="http://projecteuclid.org/download/pdf_1/euclid.aos/1176344136">Schwarz’s original 1978 manuscript</a>.</li>
<li><p>AIC and BIC are both maximum likelihood estimators that introduce penalty terms for the number of parameters in the model in an effort to combat overfitting. They do so in ways that result in significantly different behavior. Lets look at one commonly presented version of the methods (which results form stipulating normally distributed errors and other well behaving assumptions):</p>
<p>\[AIC = 2k - 2ln(L)\], and \[BIC = ln(n)k - 2ln(L)\], where:</p></li>
<li><span class="math inline">\(k\)</span> = model degrees of freedom, or free parameters to be estimated. In terms of linear regression, it is int he number of regressors including the intercept; <span class="math inline">\(\hat L\)</span> = maximized value of the likelihood function of the model <span class="math inline">\(M\)</span>, i.e., <span class="math inline">\(\hat L = p(x|\hat \theta, M)\)</span>, where <span class="math inline">\(\hat \theta\)</span> are the parameter values that maximize the likelihood function; <span class="math inline">\(n\)</span> is the number of observations.</li>
<li><p>The best model in the group compared is the one that minimizes these scores, in both cases. Clearly, AIC does not depend directly on sample size. Moreover, generally speaking, AIC presents the danger that it might over fit, whereas BIC presents the danger that it might under fit, simply in virtue of how they penalize free parameters (<span class="math inline">\(2k\)</span> in AIC; <span class="math inline">\(ln(n)k\)</span> in BIC). Diachronically, as data is introduced and the scores are recalculated, at relatively low <span class="math inline">\(n\)</span> (7 and less) BIC is more tolerant of free parameters than AIC, but less tolerant at higher <span class="math inline">\(n\)</span> (as the natural log of <span class="math inline">\(n\)</span> overcomes 2).</p></li>
</ul>
<p>AIC Results:</p>
<pre class="r"><code># Stepwise Selection based on AIC
library(MASS)
step &lt;- stepAIC(fit, direction = &quot;both&quot;)</code></pre>
<pre><code>## Start:  AIC=83.59
## mpg ~ cyl + drat + qsec + vs + am + gear
## 
##        Df Sum of Sq    RSS    AIC
## - gear  2     8.145 256.61 80.618
## - qsec  1     1.577 250.04 81.789
## - drat  1     3.761 252.23 82.067
## - vs    1     4.284 252.75 82.133
## &lt;none&gt;              248.47 83.586
## - cyl   2    35.897 284.36 83.905
## - am    1    33.994 282.46 85.690
## 
## Step:  AIC=80.62
## mpg ~ cyl + drat + qsec + vs + am
## 
##        Df Sum of Sq    RSS    AIC
## - drat  1     0.561 257.17 78.688
## - qsec  1     2.106 258.72 78.880
## - vs    1     3.217 259.83 79.017
## &lt;none&gt;              256.61 80.618
## - am    1    26.346 282.96 81.746
## - cyl   2    45.366 301.98 81.828
## + gear  2     8.145 248.47 83.586
## 
## Step:  AIC=78.69
## mpg ~ cyl + qsec + vs + am
## 
##        Df Sum of Sq    RSS    AIC
## - qsec  1     1.723 258.89 76.902
## - vs    1     3.427 260.60 77.112
## &lt;none&gt;              257.17 78.688
## - am    1    30.494 287.67 80.274
## + drat  1     0.561 256.61 80.618
## - cyl   2    58.713 315.88 81.269
## + gear  2     4.945 252.23 82.067
## 
## Step:  AIC=76.9
## mpg ~ cyl + vs + am
## 
##        Df Sum of Sq    RSS    AIC
## - vs    1     5.601 264.50 75.587
## &lt;none&gt;              258.90 76.902
## + qsec  1     1.723 257.17 78.688
## + drat  1     0.178 258.72 78.880
## - am    1    41.122 300.02 79.619
## + gear  2     6.300 252.60 80.114
## - cyl   2    94.591 353.49 82.867
## 
## Step:  AIC=75.59
## mpg ~ cyl + am
## 
##        Df Sum of Sq    RSS     AIC
## &lt;none&gt;              264.50  75.587
## + vs    1      5.60 258.90  76.902
## + qsec  1      3.90 260.60  77.112
## + drat  1      0.17 264.32  77.566
## - am    1     36.77 301.26  77.752
## + gear  2      6.74 257.76  78.761
## - cyl   2    456.40 720.90 103.672</code></pre>
<pre class="r"><code>panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(summary(step))</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">24.8</td>
<td align="center">1.323</td>
<td align="center">18.75</td>
<td align="center">2.182e-17</td>
</tr>
<tr class="even">
<td align="center"><strong>cyl6</strong></td>
<td align="center">-6.156</td>
<td align="center">1.536</td>
<td align="center">-4.009</td>
<td align="center">0.0004106</td>
</tr>
<tr class="odd">
<td align="center"><strong>cyl8</strong></td>
<td align="center">-10.07</td>
<td align="center">1.452</td>
<td align="center">-6.933</td>
<td align="center">1.547e-07</td>
</tr>
<tr class="even">
<td align="center"><strong>am1</strong></td>
<td align="center">2.56</td>
<td align="center">1.298</td>
<td align="center">1.973</td>
<td align="center">0.05846</td>
</tr>
</tbody>
</table>
<table>
<caption>Fitting linear model: mpg ~ cyl + am</caption>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">32</td>
<td align="center">3.073</td>
<td align="center">0.7651</td>
<td align="center">0.7399</td>
</tr>
</tbody>
</table>
<p>BIC Results:</p>
<pre class="r"><code># Stepwise Selection with BIC
n = dim(dat3)[1]
stepBIC = stepAIC(fit,k = log(n))</code></pre>
<pre><code>## Start:  AIC=96.78
## mpg ~ cyl + drat + qsec + vs + am + gear
## 
##        Df Sum of Sq    RSS    AIC
## - gear  2     8.145 256.61 90.879
## - qsec  1     1.577 250.04 93.515
## - drat  1     3.761 252.23 93.793
## - vs    1     4.284 252.75 93.859
## - cyl   2    35.897 284.36 94.165
## &lt;none&gt;              248.47 96.778
## - am    1    33.994 282.46 97.416
## 
## Step:  AIC=90.88
## mpg ~ cyl + drat + qsec + vs + am
## 
##        Df Sum of Sq    RSS    AIC
## - drat  1     0.561 257.17 87.483
## - qsec  1     2.106 258.72 87.674
## - vs    1     3.217 259.83 87.812
## - cyl   2    45.366 301.98 89.156
## - am    1    26.346 282.96 90.540
## &lt;none&gt;              256.61 90.879
## 
## Step:  AIC=87.48
## mpg ~ cyl + qsec + vs + am
## 
##        Df Sum of Sq    RSS    AIC
## - qsec  1     1.723 258.89 84.231
## - vs    1     3.427 260.60 84.441
## - cyl   2    58.713 315.88 87.131
## &lt;none&gt;              257.17 87.483
## - am    1    30.494 287.67 87.603
## 
## Step:  AIC=84.23
## mpg ~ cyl + vs + am
## 
##        Df Sum of Sq    RSS    AIC
## - vs    1     5.601 264.50 81.450
## &lt;none&gt;              258.90 84.231
## - am    1    41.122 300.02 85.482
## - cyl   2    94.591 353.49 87.265
## 
## Step:  AIC=81.45
## mpg ~ cyl + am
## 
##        Df Sum of Sq    RSS     AIC
## &lt;none&gt;              264.50  81.450
## - am    1     36.77 301.26  82.149
## - cyl   2    456.40 720.90 106.604</code></pre>
<pre class="r"><code>panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(summary(stepBIC))</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">Estimate</th>
<th align="center">Std. Error</th>
<th align="center">t value</th>
<th align="center">Pr(&gt;|t|)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>(Intercept)</strong></td>
<td align="center">24.8</td>
<td align="center">1.323</td>
<td align="center">18.75</td>
<td align="center">2.182e-17</td>
</tr>
<tr class="even">
<td align="center"><strong>cyl6</strong></td>
<td align="center">-6.156</td>
<td align="center">1.536</td>
<td align="center">-4.009</td>
<td align="center">0.0004106</td>
</tr>
<tr class="odd">
<td align="center"><strong>cyl8</strong></td>
<td align="center">-10.07</td>
<td align="center">1.452</td>
<td align="center">-6.933</td>
<td align="center">1.547e-07</td>
</tr>
<tr class="even">
<td align="center"><strong>am1</strong></td>
<td align="center">2.56</td>
<td align="center">1.298</td>
<td align="center">1.973</td>
<td align="center">0.05846</td>
</tr>
</tbody>
</table>
<table>
<caption>Fitting linear model: mpg ~ cyl + am</caption>
<thead>
<tr class="header">
<th align="center">Observations</th>
<th align="center">Residual Std. Error</th>
<th align="center"><span class="math inline">\(R^2\)</span></th>
<th align="center">Adjusted <span class="math inline">\(R^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">32</td>
<td align="center">3.073</td>
<td align="center">0.7651</td>
<td align="center">0.7399</td>
</tr>
</tbody>
</table>
<ul>
<li>As you can see, both AIC and BIC methods produce very similar results, and suggest the cyl6, cyl8, and am1 variables provide the least loss of information.</li>
</ul>
</div>
<div id="standardize-coefficients" class="section level3">
<h3>Standardize Coefficients</h3>
<ul>
<li>Standardization or standardized coefficients (i.e. estimates) is important when the independent variables are of different units. In our case, mpg, cyl, disp, etc. all have different unit of measure. Therefore, if we were to rank their importance in the model, it would not be a fair comparison.</li>
<li>The absolute value of standardized coefficients is used to rank independent variables, of which the maximum is most important.</li>
</ul>
<pre class="r"><code># R Function : Standardised coefficients
stdz.coff &lt;- function (regmodel)
{ b &lt;- summary(regmodel)$coef[-1,1]
sx &lt;- sapply(regmodel$model[-1], sd)
sy &lt;- sapply(regmodel$model[1], sd)
beta &lt;- b * sx / sy
return(beta)
}

std.Coeff = data.frame(Standardized.Coeff = stdz.coff(stepBIC))

std.Coeff = cbind(Variable = row.names(std.Coeff), std.Coeff)
row.names(std.Coeff) = NULL
pander(std.Coeff)</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">Variable</th>
<th align="center">Standardized.Coeff</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">cyl6</td>
<td align="center">-0.9121</td>
</tr>
<tr class="even">
<td align="center">cyl8</td>
<td align="center">-0.8335</td>
</tr>
<tr class="odd">
<td align="center">am1</td>
<td align="center">0.3793</td>
</tr>
</tbody>
</table>
<ul>
<li>Thus, in our data, cyl6 is the most important variable.</li>
</ul>
</div>
<div id="multicollinearity" class="section level3">
<h3>Multicollinearity</h3>
<ul>
<li>The variance inflation factors (VIF) function from the {car} package is used to calculate the variance-inflation and generalized variance-inflation factors for linear and generalized linear models.</li>
<li>Variables with 1 degree of freedom (in our case, am) are calculated with the usual VIF, whereas, variables with more than one degree of freedom are calculated with generalized VIF.</li>
<li>VIF for a single explanatory is obtained using the <span class="math inline">\(R^2\)</span> value of the regression of that variable against all other explanatory variables: <span class="math inline">\(VIF_j = \frac {1}{1-R_j^2}\)</span>, where <span class="math inline">\(VIF\)</span> for variable <span class="math inline">\(j\)</span> is the reciprocal of the inverse of <span class="math inline">\(R^2\)</span> from the regression. - As a rule of thumb, if <span class="math inline">\(VIF &gt; 10\)</span>, then multicollinearity is high. For more info see this <a href="https://www.r-bloggers.com/collinearity-and-stepwise-vif-selection/">blog</a>.</li>
</ul>
<pre class="r"><code># Multicollinearity Test (Should be &lt; 10)
# Check VIF of all the variables
panderOptions(&quot;table.style&quot;, &quot;simple&quot;)
pander(vif(stepBIC))</code></pre>
<table>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center">GVIF</th>
<th align="center">Df</th>
<th align="center">GVIF^(1/(2*Df))</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>cyl</strong></td>
<td align="center">1.376</td>
<td align="center">2</td>
<td align="center">1.083</td>
</tr>
<tr class="even">
<td align="center"><strong>am</strong></td>
<td align="center">1.376</td>
<td align="center">1</td>
<td align="center">1.173</td>
</tr>
</tbody>
</table>
<ul>
<li>In our data, the variables scored very low for multicollinearity.</li>
</ul>
</div>
<div id="autocorrelation" class="section level3">
<h3>Autocorrelation</h3>
<ul>
<li>The Durbin-Watson statistic is a test for serial correlations between errors. It tests for the presence of autocorrelation (described as a relationship between values that is separated from each other by a given time lag) in the residuals (prediction errors) from a regression analysis.</li>
<li>The test varies between 0 and 4, with the value of 2 indicated uncorrelated variables. Greater than 2 is a negative correlation between adjacent residuals, whereas a value less than 2 is a positive correlation. If the score is less than 1, there may be cause for alarm.</li>
<li>If the <span class="math inline">\(e_t\)</span> is the residual associated with the observation at time <span class="math inline">\(t\)</span>, then the test statistic is: <span class="math inline">\(d = \frac{\sum_{t=2}^T (e_t - e_{t-1})^2 }{\sum_{t=1}^T e_t^2}\)</span>, where <span class="math inline">\(T\)</span> is the number of observations.</li>
</ul>
<pre class="r"><code># Autocorrelation Test
durbinWatsonTest(stepBIC)</code></pre>
<pre><code>##  lag Autocorrelation D-W Statistic p-value
##    1       0.1227506      1.619958   0.196
##  Alternative hypothesis: rho != 0</code></pre>
<ul>
<li>In our data, the 1.6 D-W Statistic shows a small positive correlation, that is not statistically significant.</li>
</ul>
</div>
<div id="normality-of-residuals" class="section level3">
<h3>Normality of Residuals</h3>
<ul>
<li>The Shapiro-Wilk test is a test of normality, which utilizes the null hypothesis principle to check whether a sample <span class="math inline">\(x_1,...x_n\)</span> came form a normally distributed population: <span class="math inline">\(W = \frac{(\sum_{i=1}^n a_ix_{(i)})^2 }{\sum_{i=1}^n (x_i - \overline x)^2}\)</span>, where <span class="math inline">\(x(i)\)</span> is the <span class="math inline">\(ith\)</span> order statistic (smallest number in the sample) and <span class="math inline">\(\overline x = (x_1+...+x_n)/n\)</span> is the sample mean and the <span class="math inline">\(a_i\)</span> are given by <span class="math inline">\((a_1,...,a_n) = \frac{m^TV^{-1}}{(m^TV^{-1}V^{-1}m)^{1/2}}\)</span>, where <span class="math inline">\(m = (m_1,...m_n)^T\)</span>, and <span class="math inline">\(m_1,...,m_n\)</span> are the expected values of the order statistics of independent and identically distributed random variables sampled from the standard normal distribution, and <span class="math inline">\(V\)</span> is the covariance matrix of those order statistics.</li>
</ul>
<pre class="r"><code># Normality Of Residuals (Should be &gt; 0.05)
res = residuals(stepBIC, type = &quot;pearson&quot;)
fit.standards &lt;- rstandard(stepBIC)

qqnorm(fit.standards, ylab = &quot;Standardized Residuals&quot;, xlab = &quot;Normal Scores&quot;, main = &quot;Q-Q Plot: Model (mpg ~.)&quot;) 
qqline(fit.standards)</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-14-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre class="r"><code>shapiro.test(res)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  res
## W = 0.98002, p-value = 0.8001</code></pre>
<ul>
<li>For our data, the Shapiro-Wilk test is not significant and we can not reject the null hypothesis that the data tested are from a normally distributed population.</li>
</ul>
</div>
<div id="heteroscedasticity" class="section level3">
<h3>Heteroscedasticity</h3>
<ul>
<li>The ncvTest function tests whether the variance of the errors from a regression is dependent on the values of the independent variables.<br />
</li>
<li>The test is often called <a href="https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test">Breusch-Pagan</a> test, which returns a chisqTest object. The procedures involve some lengthy equations. Also, check out this <a href="https://www.r-bloggers.com/heteroscedasticity/">blog</a> for more information.</li>
</ul>
<pre class="r"><code>library(lmtest)
# Heteroscedasticity test (Should be &gt; 0.05)
ncvTest(stepBIC)</code></pre>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 3.40123    Df = 1     p = 0.06514782</code></pre>
<pre class="r"><code># bptest(stepBIC) with the lmtest package the following matrix can be constructed
coeftest(stepBIC,vcov=hccm(stepBIC)) </code></pre>
<pre><code>## 
## t test of coefficients:
## 
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept)  24.8019     1.2554 19.7568 &lt; 2.2e-16 ***
## cyl6         -6.1561     1.3282 -4.6349 7.522e-05 ***
## cyl8        -10.0676     1.4142 -7.1191 9.557e-08 ***
## am1           2.5600     1.1237  2.2782   0.03054 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># The actual model can be plotted to see all 4 charts at once.
par(mfrow = c(2,2)) # init 4 charts in 1 panel
plot(stepBIC)</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-15-1.png" width="576" style="display: block; margin: auto;" /></p>
<ul>
<li>For our data, the Non-constant variance score test has a p-value of 0.065, and therefore we can not reject the null hypothesis.</li>
</ul>
</div>
<div id="outliers" class="section level3">
<h3>Outliers</h3>
<ul>
<li>The outlierTest function reports the Bonferroni p-values for Studentized residuals in linear and generalized linear models, based on a t-test for linear models and normal-distribution test for generalized linear models.</li>
<li>For a very in depth look at outliers see this <a href="http://r-statistics.co/Outlier-Treatment-With-R.html">blog</a>. Further, this <a href="http://data.library.virginia.edu/diagnostic-plots/">site</a> helps to explain the interpretation of many these diagnostic plots.</li>
</ul>
<pre class="r"><code># Outliers – Bonferonni test
outlierTest(stepBIC)</code></pre>
<pre><code>## 
## No Studentized residuals with Bonferonni p &lt; 0.05
## Largest |rstudent|:
##    rstudent unadjusted p-value Bonferonni p
## 20  2.43792           0.021635      0.69232</code></pre>
<pre class="r"><code># Residuals
resid &lt;- residuals(stepBIC)
leveragePlots(stepBIC)</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-16-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
<div id="relative-importance" class="section level3">
<h3>Relative Importance</h3>
<pre class="r"><code># Relative Importance
library(relaimpo)
calc.relimp(stepBIC)</code></pre>
<pre><code>## Response variable: mpg 
## Total response variance: 36.3241 
## Analysis based on 32 observations 
## 
## 3 Regressors: 
## Some regressors combined in groups: 
##         Group  cyl : cyl6 cyl8 
## 
##  Relative importance of 2 (groups of) regressors assessed: 
##  cyl am 
##  
## Proportion of variance explained by model: 76.51%
## Metrics are not normalized (rela=FALSE). 
## 
## Relative importance metrics: 
## 
##           lmg
## cyl 0.5688862
## am  0.1962251
## 
## Average coefficients for different model sizes: 
## 
##          1group    2groups
## cyl6  -6.920779  -6.156118
## cyl8 -11.563636 -10.067560
## am     7.244939   2.559954</code></pre>
<pre class="r"><code># See Predicted Value
pred = predict(stepBIC,dat3)

#See Actual vs. Predicted Value
finaldata = cbind(model, pred)
print(head(subset(finaldata, select = c(mpg, pred))))</code></pre>
<pre><code>##     mpg     pred
## 1: 21.0 21.20569
## 2: 21.0 21.20569
## 3: 22.8 27.36181
## 4: 21.4 18.64573
## 5: 18.7 14.73429
## 6: 18.1 18.64573</code></pre>
<pre class="r"><code># As for the outlier that was identified above
print(subset(finaldata, select = c(mpg, pred))[20,])</code></pre>
<pre><code>##     mpg     pred
## 1: 33.9 27.36181</code></pre>
</div>
<div id="manual-calculation-of-rmse-rsquared-and-adjusted-rsquared" class="section level3">
<h3>Manual Calculation of RMSE, Rsquared and Adjusted Rsquared</h3>
<ul>
<li>For an in depth look at Box-Cox transformations see this <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=5&amp;ved=0ahUKEwi-uu-vu8HRAhVDPiYKHSCSAkUQFgg1MAQ&amp;url=https%3A%2F%2Fwww.ime.usp.br%2F~abe%2Flista%2Fpdfm9cJKUmFZp.pdf&amp;usg=AFQjCNEJL7m6IfYTgRohn8Z8dVLWphMjgA">site</a>.</li>
</ul>
<pre class="r"><code>#Calculating RMSE
rmse &lt;- sqrt(mean((dat3$mpg - pred)^2))
print(rmse)</code></pre>
<pre><code>## [1] 2.874977</code></pre>
<pre class="r"><code>#Calculating Rsquared manually
y = as.list(dat3[,c(&quot;mpg&quot;)])
R.squared = 1 - sum((y$mpg - pred)^2)/sum((y$mpg - mean(y$mpg))^2)
print(R.squared)</code></pre>
<pre><code>## [1] 0.7651114</code></pre>
<pre class="r"><code>#Calculating Adj. Rsquared manually
n = dim(dat3)[1]
p = dim(summary(stepBIC)$coeff)[1] - 1
adj.r.squared = 1 - (1 - R.squared) * ((n - 1)/(n - p - 1))
print(adj.r.squared)</code></pre>
<pre><code>## [1] 0.7399447</code></pre>
<pre class="r"><code>#Box Cox Transformation
library(lmSupport)
modelBoxCox(stepBIC)</code></pre>
<p><img src="/post/2017-01-14-linear-regression_files/figure-html/unnamed-chunk-19-1.png" width="576" style="display: block; margin: auto;" /></p>
<pre><code>## [1] &quot;Best Lambda= 0.38&quot;
## [1] &quot;Chi-square (df=1)= 3.01&quot;
## [1] &quot;p-value= 0.08285&quot;</code></pre>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<ul>
<li>Linear regression has been around for a long time, and is still very important for quantitative predictions. Frankly, this blog only scratches the surface on linear regression. At each step, there are many ways to conduct each analysis. Personally, I look forward to learning more and more ways to approach linear regression, as it is a foundational to many other tests that are more sophisticated. Until next time…</li>
</ul>
</div>

					</div>
				</article>

			</div>
		</div> 

        <div class="row">
			<div class="col-sm-8 col-sm-offset-2">

				<div id="share">
                    
				</div>
			</div>
		</div> 
		<div class="clearfix"></div>

		<div class="row">
			<div class="col-sm-8 col-sm-offset-2">

				<div id="comments">
                    
				</div>
			</div>
		</div> 
		<div class="clearfix"></div>

	</div>	

</main>
<div id="disqus_thread"></div>
<script>





(function() { 
var d = document, s = d.createElement('script');
s.src = 'https://https-jdstallings-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<footer id="footer">
	<div class="container">
		<div class="row">
			<div class="col-md-3 widget">
				<h3 class="widget-title">Contact</h3>
				<div class="widget-body">
					<p>&#43;1 253-861-0509<br>
						<a href="mailto:jdstallings@DataInDeed.io">jdstallings@DataInDeed.io</a><br>
						<br>
						Frederick, MD 21702
					</p>
				</div>
			</div>

			<div class="col-md-3 widget">
				<h3 class="widget-title">Follow me</h3>
				<div class="widget-body">
					<p class="follow-me-icons">
                        
                            
                                <a href="https://www.facebook.com/DataIndeed" target="_blank"><i class="fa fa-facebook-square fa-2"></i></a>
                            
                        
                            
                                <a href="https://twitter.com/dataindeed" target="_blank"><i class="fa fa-twitter-square fa-2"></i></a>
                            
                        
                            
                                <a href="https://www.instagram.com/dataindeed" target="_blank"><i class="fa fa-instagram fa-2"></i></a>
                            
                        
                            
                        
                            
                                <a href="https://www.linkedin.com/in/jonathandstallings" target="_blank"><i class="fa fa-linkedin-square fa-2"></i></a>
                            
                        
                            
                                <a href="https://github.com/jdstallings" target="_blank"><i class="fa fa-github fa-2"></i></a>
                            
                        
                            
                                <a href="mailto:jdstallings@dataindeed.io" target="_blank"><i class="fa fa-envelope fa-2"></i></a>
                            
                        
					</p>
				</div>
			</div>

      <div class="col-md-3 widget">
        <h3 class="widget-title">SERVICES</h3>
				<div class="widget-body">
				</div>
										<form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
<input type="hidden" name="cmd" value="_s-xclick">
<input type="hidden" name="hosted_button_id" value="QFTCM5WQPHDD8">
<input type="image" src="https://www.paypalobjects.com/en_US/i/btn/btn_buynowCC_LG.gif" border="0" name="submit" alt="PayPal - The safer, easier way to pay online!">
<img alt="" border="0" src="https://www.paypalobjects.com/en_US/i/scr/pixel.gif" width="1" height="1">
</form>

			</div>
			<div class="col-md-3 widget">
				<div class="widget-body">
					<p class="text-right">
						Copyright &copy; 2017 <br> Jonathan D. Stallings<br>
						Design: <a href="http://www.gettemplate.com" rel="designer">Initio by GetTemplate</a>
            Powered by: <a href="https://gohugo.io/" rel="poweredby">Hugo</a></p>
				</div>
			</div>
		</div> 
	</div>
</footer>

<footer id="underfooter">
	<div class="container">
		<div class="row">

			<div class="col-md-6 widget">
				<div class="widget-body">
					<p></p>
				</div>
			</div>

		</div> 
	</div>
</footer>

			<div class="col-md-3 widget">
			</div>



<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="/js/template.js"></script>
<script id="dsq-count-scr" src="//https://https-jdstallings-github-io.disqus.com/count.js" async></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-114986818-1', 'auto');
  ga('send', 'pageview');
  
</script>

</body>
</html>

<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

